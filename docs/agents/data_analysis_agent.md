# Agent3: Data Analysis Agent

## Descripci√≥n General

El `DataAnalysisAgent` es un agente inteligente especializado en an√°lisis de datos que combina tres tecnolog√≠as clave:

1. **Memoria Conversacional**
2. **Estructura LangGraph por Nodos**
3. **Capacidades de An√°lisis de Datos** (basado en `guia_agent3_data_analysis`)

## Caracter√≠sticas Principales

### üß† Memoria Conversacional
- Mantiene el contexto de conversaciones por sesi√≥n
- Permite referencias a an√°lisis anteriores ("como vimos antes", "comparado con el resultado anterior")
- Soporte para m√∫ltiples sesiones independientes
- Gesti√≥n autom√°tica del historial de consultas

### üîÑ Flujo Estructurado con LangGraph
El agente procesa las consultas a trav√©s de 5 nodos especializados:

1. **Input Processing**: Analiza la intenci√≥n de la consulta
2. **Data Preparation**: Valida y prepara el dataset
3. **Analysis Execution**: Ejecuta el an√°lisis usando pandas
4. **Result Formatting**: Formatea la respuesta de manera comprensible
5. **Memory Update**: Actualiza la memoria de la sesi√≥n

### üìä An√°lisis de Datos Avanzado
- Integraci√≥n con `langchain_experimental.agents.agent_toolkits.create_pandas_dataframe_agent`
- Soporte para CSV, Excel y DataFrames de pandas
- An√°lisis estad√≠sticos autom√°ticos
- Respuestas en lenguaje natural

## Instalaci√≥n y Configuraci√≥n

### Dependencias
```bash
pip install -r requirements.txt
```

Las dependencias principales incluyen:
- `langchain>=0.1.0`
- `langchain-experimental>=0.1.0`
- `langgraph>=0.1.0`
- `pandas>=1.5.0`
- `numpy>=1.21.0`

### Variables de Entorno
Crear un archivo `.env` con:
```
OPENAI_API_KEY=tu_api_key_aqui
```

## Uso B√°sico

### Inicializaci√≥n
```python
from agents.agent3_data_analysis import DataAnalysisAgent

# Crear e inicializar el agente
agent = DataAnalysisAgent()
agent.initialize()
```

### Consultas Simples
```python
# Hacer una consulta b√°sica
response = agent.respond("¬øCu√°ntas filas tiene el dataset?", "mi_sesion")
print(response)
```

### Cargar Dataset Personalizado
```python
import pandas as pd

# Cargar desde archivo
result = agent.load_dataset("mi_archivo.csv")

# O desde DataFrame
df = pd.read_csv("datos.csv")
result = agent.load_dataset(dataframe=df)
```

## Ejemplos de Uso

### An√°lisis B√°sico
```python
# Informaci√≥n general
agent.respond("¬øCu√°les son las columnas del dataset?", "sesion1")
agent.respond("¬øCu√°ntas filas tiene?", "sesion1")
agent.respond("Dame un resumen estad√≠stico", "sesion1")

# An√°lisis espec√≠fico
agent.respond("¬øCu√°l es el precio promedio?", "sesion1")
agent.respond("¬øQu√© marca aparece m√°s frecuentemente?", "sesion1")
```

### Memoria Conversacional
```python
# Primera consulta
agent.respond("¬øCu√°l es el precio promedio de los autos?", "sesion1")

# Consulta que hace referencia a la anterior
agent.respond("¬øY cu√°l es el precio m√°ximo?", "sesion1")

# Consulta comparativa
agent.respond("Compara esos dos valores", "sesion1")
```

### M√∫ltiples Sesiones
```python
# Sesi√≥n para an√°lisis de precios
agent.respond("Analiza los precios", "precios")

# Sesi√≥n independiente para marcas
agent.respond("Analiza las marcas", "marcas")

# Las sesiones mantienen contexto independiente
agent.respond("¬øCu√°l era el promedio?", "precios")  # Se refiere a precios
agent.respond("¬øCu√°l era la m√°s com√∫n?", "marcas")   # Se refiere a marcas
```

## M√©todos Disponibles

### M√©todos Principales
- `initialize()`: Inicializa el agente
- `respond(message, session_id)`: Procesa una consulta
- `load_dataset(file_path, dataframe)`: Carga un dataset
- `get_dataset_info()`: Informaci√≥n del dataset actual

### Gesti√≥n de Memoria
- `get_session_history(session_id)`: Obtiene historial de una sesi√≥n
- `clear_session_memory(session_id)`: Limpia memoria de una sesi√≥n
- `get_available_sessions()`: Lista sesiones activas

## Tipos de An√°lisis Soportados

### Descriptivos
- Estad√≠sticas b√°sicas (media, mediana, moda)
- Informaci√≥n de columnas y tipos de datos
- Conteo de valores √∫nicos
- Detecci√≥n de valores nulos

### Comparativos
- Comparaciones entre grupos
- Correlaciones entre variables
- An√°lisis por categor√≠as

### Temporales
- Tendencias a lo largo del tiempo
- An√°lisis por per√≠odos
- Estacionalidad

### Filtrado y Agregaci√≥n
- Filtros por condiciones espec√≠ficas
- Agrupaciones por categor√≠as
- Operaciones de suma, promedio, etc.

## Dataset de Ejemplo

El agente incluye un dataset de ejemplo con 1000 registros de ventas de autos:

**Columnas:**
- `Date`: Fecha de venta
- `Make`: Marca del auto
- `Model`: Tipo de modelo
- `Color`: Color del auto
- `Year`: A√±o del auto
- `Price`: Precio de venta
- `Mileage`: Kilometraje
- `EngineSize`: Tama√±o del motor
- `FuelEfficiency`: Eficiencia de combustible
- `SalesPerson`: Vendedor

## Estructura Interna

### Estado de LangGraph
```python
class DataAnalysisState(TypedDict):
    query: str                      # Consulta original
    dataset_info: Dict[str, Any]    # Informaci√≥n del dataset
    analysis_result: str            # Resultado del an√°lisis
    conversation_history: List[str] # Historial de la sesi√≥n
    session_id: str                 # ID de sesi√≥n
    final_response: str             # Respuesta final
    dataframe: Optional[pd.DataFrame] # DataFrame actual
    pandas_agent: Optional[Any]     # Agente pandas
    error_message: Optional[str]    # Mensajes de error
```

### Flujo de Nodos
1. **Input Processing**: Clasifica la intenci√≥n (descriptive, comparative, temporal, etc.)
2. **Data Preparation**: Valida dataset y extrae metadatos
3. **Analysis Execution**: Ejecuta an√°lisis con pandas agent
4. **Result Formatting**: Mejora formato de respuesta con LLM
5. **Memory Update**: Guarda en historial de sesi√≥n

## Comandos de Demo

### Ejecutar Demo Completo
```bash
cd src
python demo_agent3.py
```

### Ejecutar Tests
```bash
cd src
python tests/test_agent3.py
```

### Ejecutar desde Main
```bash
cd src
python main.py
# Seleccionar opci√≥n 4
```

## Casos de Uso Detallados

### üîç Ejemplo 1: An√°lisis Exploratorio Completo

**Escenario:** Analizar ventas de una empresa desde archivo CSV

```python
# Inicializaci√≥n
agent = DataAnalysisAgent()
await agent.initialize()

# Sesi√≥n de an√°lisis exploratorio
session = "analisis_ventas_2024"

# 1. Carga de datos
response1 = await agent.respond(
    "Carga el archivo ventas_2024.csv", 
    session
)
print(response1.content)
# Output: "Datos cargados exitosamente:
#         - Archivo: ventas_2024.csv
#         - Filas: 1,250
#         - Columnas: 8
#         - Columnas: fecha, producto, ventas, region, vendedor, categoria, precio_unitario, descuento"

# 2. Perfil general del dataset
response2 = await agent.respond(
    "Dame un resumen general del dataset",
    session
)
# Output: Informaci√≥n sobre tipos de datos, valores faltantes, estad√≠sticas b√°sicas

# 3. An√°lisis espec√≠fico
response3 = await agent.respond(
    "¬øCu√°l es la regi√≥n con mayores ventas?",
    session
)

# 4. An√°lisis temporal
response4 = await agent.respond(
    "Mu√©strame las tendencias de ventas por mes",
    session
)

# 5. An√°lisis comparativo (usando memoria conversacional)
response5 = await agent.respond(
    "Compara las ventas de esa regi√≥n con las otras",
    session  # El agente recuerda cu√°l era "esa regi√≥n"
)
```

### üìä Ejemplo 2: An√°lisis de Correlaciones

**Escenario:** Entender relaciones entre variables

```python
session = "correlaciones_productos"

# 1. Cargar datos de muestra
await agent.respond("Crea datos de muestra de productos", session)

# 2. An√°lisis de correlaciones
response = await agent.respond(
    "Analiza las correlaciones entre precio, stock y ventas",
    session
)

# El sistema ejecuta internamente:
# - stats_processor.correlation_analysis()
# - Genera matriz de correlaci√≥n
# - Identifica correlaciones significativas
# - Explica las relaciones en lenguaje natural

print(response.content)
# Output: "An√°lisis de correlaciones completado:
#         
#         Correlaciones significativas encontradas:
#         - Precio vs Stock: correlaci√≥n negativa (-0.65)
#         - Stock vs Ventas: correlaci√≥n positiva (0.78)
#         
#         Interpretaci√≥n:
#         - Los productos m√°s caros tienden a tener menos stock
#         - Mayor stock se asocia con mayores ventas"
```

### üìà Ejemplo 3: Detecci√≥n de Anomal√≠as

```python
session = "deteccion_anomalias"

# 1. An√°lisis de outliers
response = await agent.respond(
    "Detecta valores at√≠picos en las ventas diarias",
    session
)

# El sistema internamente:
# 1. Aplica m√©todo IQR (Interquartile Range)
# 2. Identifica valores fuera de Q1 - 1.5*IQR y Q3 + 1.5*IQR
# 3. Analiza patrones en los outliers
# 4. Sugiere posibles explicaciones

print(response.content)
# Output: "Outliers detectados en ventas diarias:
#         
#         - 5 valores at√≠picos encontrados
#         - Fechas: 2024-03-15, 2024-07-04, 2024-12-25, 2024-12-31, 2024-01-01
#         - Valores: $45,231 (M√°x), $892 (M√≠n)
#         
#         Posibles explicaciones:
#         - Picos en fechas especiales (Navidad, A√±o Nuevo)
#         - Ca√≠da en d√≠a festivo nacional
#         
#         Recomendaci√≥n: Considerar estacionalidad en an√°lisis futuros"
```

### üéØ Ejemplo 4: An√°lisis Guiado por Preguntas

**Flujo conversacional inteligente:**

```python
session = "analisis_guiado"

# El agente mantiene contexto entre preguntas
queries = [
    "¬øCu√°l es el producto m√°s vendido?",
    "¬øEn qu√© regiones se vende m√°s?", 
    "¬øCu√°l es la estacionalidad de ese producto?",
    "¬øQu√© factores influyen en sus ventas?",
    "¬øHay diferencias por vendedor?",
    "Bas√°ndote en todo esto, ¬øqu√© recomendaciones dar√≠as?"
]

for i, query in enumerate(queries, 1):
    print(f"\n--- Pregunta {i} ---")
    response = await agent.respond(query, session)
    print(f"Q: {query}")
    print(f"A: {response.content}")
    
# El agente recuerda:
# - "ese producto" se refiere al m√°s vendido mencionado antes
# - "todo esto" incluye el an√°lisis previo completo
# - Construye recomendaciones basadas en la conversaci√≥n completa
```

### üîÑ Ejemplo 5: M√∫ltiples Sesiones Independientes

```python
# Sesi√≥n 1: An√°lisis de ventas
await agent.respond("Analiza ventas por trimestre", "ventas_q1_q4")
await agent.respond("¬øCu√°l fue el mejor trimestre?", "ventas_q1_q4")

# Sesi√≥n 2: An√°lisis de inventario (independiente)
await agent.respond("Analiza niveles de stock", "inventario_2024")
await agent.respond("¬øQu√© productos est√°n en stock bajo?", "inventario_2024")

# Sesi√≥n 3: An√°lisis de clientes (independiente)
await agent.respond("Segmenta clientes por valor", "segmentacion_clientes")
await agent.respond("¬øCu√°l es el perfil del cliente premium?", "segmentacion_clientes")

# Las preguntas "¬øCu√°l fue..." en cada sesi√≥n se refieren 
# al contexto espec√≠fico de esa sesi√≥n √∫nicamente
```

### üõ†Ô∏è Ejemplo 6: An√°lisis con Datos Personalizados

**Preparaci√≥n de datos propios:**

```python
# 1. Estructura recomendada para tu archivo CSV
"""
fecha,cliente,producto,cantidad,precio_total,descuento,vendedor,region
2024-01-15,CLIENTE_001,LAPTOP_GAMING,2,2400.00,5.0,Santiago,Norte
2024-01-15,CLIENTE_002,MOUSE_WIRELESS,1,45.90,0.0,Mar√≠a,Sur
2024-01-16,CLIENTE_003,TECLADO_MECANICO,1,120.50,10.0,Carlos,Este
"""

# 2. Carga y an√°lisis
session = "mi_analisis_personalizado"

# Cargar tus datos
await agent.respond(
    "Carga el archivo mi_dataset_ventas.csv", 
    session
)

# An√°lisis espec√≠ficos de tu negocio
await agent.respond(
    "¬øCu√°l es el ticket promedio por cliente?",
    session
)

await agent.respond(
    "¬øQu√© vendedor tiene mejor performance?",
    session
)

await agent.respond(
    "¬øHay patrones estacionales en las ventas?",
    session
)

await agent.respond(
    "¬øEl descuento impacta en el volumen de ventas?",
    session
)
```

### üìã Ejemplo 7: Reportes Automatizados

```python
session = "reporte_mensual"

# El agente puede generar reportes estructurados
response = await agent.respond(
    """Genera un reporte completo que incluya:
    1. Resumen ejecutivo de ventas
    2. Top 5 productos por ventas
    3. Performance por regi√≥n
    4. An√°lisis de tendencias
    5. Recomendaciones de acci√≥n""",
    session
)

# El sistema estructura autom√°ticamente la respuesta
print(response.content)
# Output:
# """
# REPORTE MENSUAL DE VENTAS - ENERO 2024
# =====================================
# 
# 1. RESUMEN EJECUTIVO
# - Ventas totales: $2,450,000 (+15% vs mes anterior)
# - Unidades vendidas: 1,250 (+8% vs mes anterior)
# - Ticket promedio: $1,960 (+6% vs mes anterior)
# 
# 2. TOP 5 PRODUCTOS
# 1. Laptop Gaming X1: $850,000 (34.7% del total)
# 2. Monitor 4K Pro: $420,000 (17.1% del total)
# ...
# 
# 3. PERFORMANCE POR REGI√ìN
# - Norte: $980,000 (40% del total) - L√çDER
# - Sur: $735,000 (30% del total)
# ...
# 
# 4. TENDENCIAS IDENTIFICADAS
# - Crecimiento sostenido en electr√≥nicos premium
# - Declive en accesorios b√°sicos
# ...
# 
# 5. RECOMENDACIONES
# - Incrementar stock de Laptop Gaming X1 en regi√≥n Norte
# - Revisar estrategia de pricing en accesorios
# ...
# """
```

### üéØ Casos de Uso Empresariales

#### **E-commerce:**
```python
# An√°lisis de comportamiento de compra
"¬øCu√°l es el patr√≥n de compra por hora del d√≠a?"
"¬øQu√© productos se compran juntos frecuentemente?"
"¬øCu√°l es la estacionalidad por categor√≠a?"
```

#### **Retail:**
```python
# An√°lisis de inventario y ventas
"¬øQu√© productos tienen rotaci√≥n m√°s lenta?"
"¬øCu√°l es el punto de reorden √≥ptimo por producto?"
"¬øHay diferencias de ventas por ubicaci√≥n de tienda?"
```

#### **SaaS/Software:**
```python
# An√°lisis de m√©tricas de usuario
"¬øCu√°l es el patr√≥n de retenci√≥n por cohorte?"
"¬øQu√© features correlacionan con mayor engagement?"
"¬øCu√°l es el tiempo promedio hasta conversi√≥n?"
```

#### **Finanzas:**
```python
# An√°lisis de riesgo y performance
"¬øCu√°l es la distribuci√≥n de morosidad por segmento?"
"¬øHay estacionalidad en los pagos atrasados?"
"¬øQu√© factores predicen mejor el riesgo crediticio?"
```

### üìä Tipos de An√°lisis Soportados Detallados

#### **Descriptivos B√°sicos:**
- Medidas de tendencia central (media, mediana, moda)
- Medidas de dispersi√≥n (desviaci√≥n est√°ndar, varianza, rango)
- Distribuciones por percentiles
- Conteos y frecuencias

#### **Comparativos:**
- Comparaciones entre grupos/categor√≠as
- An√°lisis de diferencias estad√≠sticas
- Rankings y ordenamientos
- An√°lisis de proporciones

#### **Correlacionales:**
- Matriz de correlaciones completa
- Correlaciones parciales
- Identificaci√≥n de multicolinealidad
- An√°lisis de dependencias

#### **Temporales:**
- An√°lisis de tendencias
- Detecci√≥n de estacionalidad
- An√°lisis de crecimiento periodo a periodo
- Identificaci√≥n de puntos de inflexi√≥n

#### **Detecci√≥n de Anomal√≠as:**
- Outliers univariados (m√©todo IQR, Z-score)
- Outliers multivariados
- An√°lisis de valores at√≠picos temporales
- Identificaci√≥n de patrones an√≥malos

### 1. An√°lisis Exploratorio de Datos
- Usuarios no t√©cnicos pueden explorar datasets
- Obtener insights r√°pidos sin programar
- Entender estructura y calidad de los datos

### 2. Reportes Automatizados
- Generar reportes en lenguaje natural
- Comparaciones autom√°ticas entre per√≠odos
- Alertas sobre anomal√≠as en datos

### 3. An√°lisis Conversacional
- Sesiones de an√°lisis interactivo
- Refinamiento iterativo de consultas
- Exploraci√≥n guiada por preguntas

### 4. Prototipado R√°pido
- Validaci√≥n r√°pida de hip√≥tesis
- An√°lisis ad-hoc durante reuniones
- Exploraci√≥n de nuevas fuentes de datos

## Arquitectura y Funcionamiento Interno

### üèóÔ∏è Componentes del Sistema

El sistema de an√°lisis de datos est√° construido con una arquitectura modular que separa responsabilidades:

```
PandasAgent (Coordinador Principal)
‚îú‚îÄ‚îÄ Processors (Procesadores de Datos)
‚îÇ   ‚îú‚îÄ‚îÄ CSVProcessor - Manejo de archivos CSV
‚îÇ   ‚îú‚îÄ‚îÄ StatsProcessor - An√°lisis estad√≠sticos  
‚îÇ   ‚îî‚îÄ‚îÄ VisualizationProcessor - Generaci√≥n de gr√°ficos
‚îú‚îÄ‚îÄ Workflows (Flujos de LangGraph)
‚îÇ   ‚îî‚îÄ‚îÄ DataAnalysisWorkflow - Orchestaci√≥n del an√°lisis
‚îú‚îÄ‚îÄ Tools (Herramientas Especializadas)
‚îÇ   ‚îî‚îÄ‚îÄ DataAnalysisTools - Utilidades de an√°lisis
‚îî‚îÄ‚îÄ Models (Estructuras de Datos)
    ‚îú‚îÄ‚îÄ AnalysisRequest - Solicitudes de an√°lisis
    ‚îú‚îÄ‚îÄ AnalysisResult - Resultados estructurados
    ‚îî‚îÄ‚îÄ DataAnalysisState - Estado del workflow
```

### üîÑ Flujo Detallado de Procesamiento

#### 1. **Recepci√≥n de Input** (`PandasAgent.process()`)
```python
# El agente recibe un mensaje del usuario
input_data = "Analiza las ventas por regi√≥n"

# Se normaliza la entrada
if isinstance(input_data, str):
    message = input_data
    kwargs = {}
elif isinstance(input_data, dict):
    message = input_data.get('message', '')
    kwargs = {k: v for k, v in input_data.items() if k != 'message'}'}
```

#### 2. **Clasificaci√≥n de Request** (`_classify_request()`)
```python
def _classify_request(self, message: str) -> str:
    """El sistema analiza palabras clave para determinar la intenci√≥n"""
    message_lower = message.lower()
    
    # Palabras clave para diferentes tipos de operaciones
    analyze_keywords = ['analizar', 'analyze', 'estad√≠sticas', 'correlaci√≥n']
    visualize_keywords = ['gr√°fico', 'plot', 'visualizar', 'chart']
    profile_keywords = ['perfil', 'profile', 'resumen', 'overview']
    load_keywords = ['cargar', 'load', 'archivo', 'csv', 'datos']
    
    # Retorna: "analyze_data", "visualize_data", "profile_data", etc.
```

**Tipos de requests detectados:**
- `load_data` - Cargar/crear datos
- `analyze_data` - An√°lisis estad√≠stico
- `visualize_data` - Crear gr√°ficos
- `profile_data` - Perfil del dataset
- `export_results` - Exportar resultados
- `general` - Consulta general

#### 3. **Manejo Espec√≠fico por Tipo**

##### **Carga de Datos** (`_handle_data_loading()`)
```python
# Detecta origen de datos
if file_path:
    # Origen: Archivo CSV
    dataframe = await asyncio.to_thread(
        self.csv_processor.load_from_file, 
        file_path
    )
elif sample_data:
    # Origen: Datos de muestra
    result = await asyncio.to_thread(
        self.csv_processor.create_sample_data, 
        sample_data
    )
```

**Proceso de carga CSV:**
1. **Validaci√≥n**: Verifica que el archivo existe y tiene formato v√°lido
2. **Encoding Detection**: Intenta diferentes encodings (utf-8, latin1, cp1252, iso-8859-1)
3. **Carga**: Usa `pd.read_csv()` con par√°metros optimizados
4. **Metadatos**: Crea `DatasetInfo` con estad√≠sticas del dataset

##### **An√°lisis de Datos** (`_handle_data_analysis()`)
```python
# Verifica que hay datos cargados
if self.current_dataframe is None:
    return "Primero debes cargar datos antes de analizarlos."

# Ejecuta an√°lisis en paralelo
analyses = {}

# 1. Estad√≠sticas descriptivas
desc_stats = await asyncio.to_thread(
    self.stats_processor.descriptive_stats,
    self.current_dataframe,
    kwargs.get('target_columns')
)

# 2. An√°lisis de correlaci√≥n
corr_analysis = await asyncio.to_thread(
    self.stats_processor.correlation_analysis,
    self.current_dataframe
)

# 3. Compila resultados
analyses['descriptive'] = desc_stats
analyses['correlation'] = corr_analysis
```

### ÔøΩ Fuentes de Datos Soportadas

#### **1. Archivos CSV/TSV** (Fuente Principal)
```python
# Formatos soportados
supported_formats = ['.csv', '.tsv', '.txt']

# Encodings manejados autom√°ticamente
encoding_options = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']

# Uso
agent.respond("Carga el archivo ventas_2024.csv", "sesion1")
```

**Caracter√≠sticas:**
- ‚úÖ Detecci√≥n autom√°tica de delimitadores (`,`, `;`, `\t`)
- ‚úÖ Manejo robusto de diferentes encodings
- ‚úÖ Validaci√≥n de estructura y formato
- ‚úÖ Vista previa antes de carga completa
- ‚úÖ Detecci√≥n autom√°tica de tipos de datos

**Ejemplo de carga:**
```python
# El sistema intenta diferentes configuraciones autom√°ticamente
def _try_different_encodings(self, file_path):
    for encoding in ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            logger.info(f"Archivo cargado con encoding: {encoding}")
            return df
        except UnicodeDecodeError:
            continue
    
    # Fallback con manejo de errores
    return pd.read_csv(file_path, encoding='utf-8', errors='replace')
```

#### **2. Datos de Muestra** (Para Testing y Demos)
```python
# Tipos disponibles
sample_types = ["sales", "customers", "products"]

# Uso
agent.respond("Crea datos de muestra de ventas", "sesion1")
agent.respond("Genera dataset de clientes", "sesion1")
```

**Datasets de muestra incluidos:**

**Sales Dataset (1000 registros):**
```python
{
    'Date': ['2022-01-01', '2022-01-02', ...],
    'Product': ['Laptop', 'Mouse', 'Keyboard', ...],
    'Category': ['Electronics', 'Accessories', ...],
    'Price': [1250.50, 25.99, 89.90, ...],
    'Quantity': [2, 5, 1, ...],
    'Customer_ID': [1001, 1002, 1003, ...],
    'Sales_Rep': ['Alice', 'Bob', 'Charlie', ...]
}
```

**Customers Dataset (500 registros):**
```python
{
    'Customer_ID': [1000, 1001, 1002, ...],
    'Name': ['Customer_1', 'Customer_2', ...],
    'Age': [25, 34, 45, ...],
    'City': ['Madrid', 'Barcelona', 'Valencia', ...],
    'Segment': ['Premium', 'Standard', 'Basic', ...],
    'Registration_Date': ['2020-03-15', '2020-05-22', ...]
}
```

**Products Dataset (100 registros):**
```python
{
    'Product_ID': [1, 2, 3, ...],
    'Product_Name': ['Product_1', 'Product_2', ...],
    'Category': ['Electronics', 'Clothing', 'Books', ...],
    'Price': [199.99, 49.90, 15.50, ...],
    'Stock': [50, 120, 8, ...],
    'Supplier': ['Supplier_A', 'Supplier_B', ...]
}
```

#### **3. Strings CSV** (Datos en Memoria)
```python
# Para datos que vienen como texto
csv_string = """nombre,edad,ciudad
Juan,25,Madrid
Mar√≠a,30,Barcelona"""

df = csv_processor.load_from_string(csv_string)
```

#### **4. Implementaci√≥n Futura: Base de Datos JSON**

**Estructura propuesta:**
```json
{
  "ventas_2024": [
    {
      "fecha": "2024-01-01",
      "producto": "Laptop Gaming",
      "cantidad": 5,
      "precio": 1200.00,
      "region": "Norte",
      "vendedor": "Santiago"
    }
  ],
  "clientes_activos": [
    {
      "id": 1,
      "nombre": "Juan P√©rez",
      "edad": 35,
      "ciudad": "Madrid",
      "segmento": "Premium"
    }
  ],
  "inventario": [
    {
      "producto_id": "LAPTOP001",
      "nombre": "Laptop Gaming X1",
      "categoria": "Electr√≥nicos",
      "precio": 1200.00,
      "stock": 50,
      "proveedor": "TechCorp"
    }
  ]
}
```

**Ventajas de JSON Database:**
- ‚úÖ Control total sobre los datos disponibles
- ‚úÖ No depende de uploads de usuarios
- ‚úÖ M√∫ltiples datasets predefinidos
- ‚úÖ Estructura consistente y validada
- ‚úÖ F√°cil mantenimiento y actualizaci√≥n
- ‚úÖ Metadatos incluidos en la misma estructura

**Implementaci√≥n propuesta:**
```python
class JSONProcessor:
    def __init__(self, json_database_path: str):
        self.database_path = Path(json_database_path)
        self.data_cache = {}
    
    def load_from_json_db(self, dataset_name: Optional[str] = None):
        """Carga dataset espec√≠fico desde base JSON"""
        with open(self.database_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        if dataset_name:
            if dataset_name in json_data:
                return pd.DataFrame(json_data[dataset_name])
            else:
                available = list(json_data.keys())
                raise ValueError(f"Dataset '{dataset_name}' no encontrado. Disponibles: {available}")
        
        # Si no se especifica, mostrar datasets disponibles
        return list(json_data.keys())
    
    def list_available_datasets(self) -> List[str]:
        """Lista todos los datasets disponibles"""
        with open(self.database_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        return list(json_data.keys())
```

### üîç Proceso de Detecci√≥n de Fuente de Datos

```python
def _handle_data_loading(self, message: str, **kwargs):
    """El sistema determina autom√°ticamente la fuente de datos"""
    
    file_path = kwargs.get('file_path')
    sample_data = kwargs.get('sample_data')
    dataset_name = kwargs.get('dataset_name')  # Para JSON DB futuro
    
    # 1. Buscar en kwargs expl√≠citos
    if file_path:
        source = "file"
    elif sample_data:
        source = "sample"
    elif dataset_name:
        source = "json_db"
    
    # 2. Buscar en el mensaje del usuario
    else:
        words = message.split()
        for word in words:
            if word.endswith('.csv') or '/' in word or '\\' in word:
                file_path = word
                source = "file"
                break
            elif word in ['ventas', 'clientes', 'productos']:
                dataset_name = word
                source = "json_db"
                break
        else:
            # 3. Detectar intenci√≥n de datos de muestra
            if any(keyword in message.lower() for keyword in ['muestra', 'sample', 'demo', 'ejemplo']):
                sample_data = "sales"  # Default
                source = "sample"
    
    # 4. Procesar seg√∫n la fuente detectada
    if source == "file":
        result = await self.csv_processor.load_from_file(file_path)
    elif source == "sample": 
        result = await self.csv_processor.create_sample_data(sample_data)
    elif source == "json_db":
        result = await self.json_processor.load_from_json_db(dataset_name)
    else:
        return "Por favor especifica la fuente de datos: archivo CSV, datos de muestra, o dataset espec√≠fico"
```

### üìà Validaci√≥n y Calidad de Datos

**Proceso autom√°tico de validaci√≥n:**
```python
def validate_data(self, df: pd.DataFrame) -> Dict[str, Any]:
    validation_results = {
        # Informaci√≥n b√°sica
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,
        
        # Tipos de datos
        'data_types': df.dtypes.astype(str).to_dict(),
        'numeric_columns': df.select_dtypes(include='number').columns.tolist(),
        'categorical_columns': df.select_dtypes(include='object').columns.tolist(),
        'datetime_columns': df.select_dtypes(include='datetime').columns.tolist(),
        
        # Calidad de datos
        'missing_values': df.isnull().sum().to_dict(),
        'duplicate_rows': df.duplicated().sum(),
        'missing_percentage': {
            col: (count / len(df)) * 100 
            for col, count in df.isnull().sum().items()
        }
    }
    
    return validation_results
```

**Alertas de calidad generadas:**
- ‚ö†Ô∏è Columnas con >20% de valores faltantes
- ‚ö†Ô∏è Filas duplicadas encontradas
- ‚ö†Ô∏è Columnas con un solo valor √∫nico
- ‚ö†Ô∏è Posibles errores de tipo de dato
- ‚ö†Ô∏è Valores extremos detectados (outliers)


#### **CSVProcessor** - El Coraz√≥n de la Carga de Datos

**Responsabilidades:**
- Carga archivos CSV/TSV/TXT
- Detecci√≥n autom√°tica de delimitadores
- Manejo robusto de encodings
- Validaci√≥n de calidad de datos
- Generaci√≥n de datos de muestra

**Flujo de carga de archivo:**
```python
def load_from_file(self, file_path):
    # 1. Validaci√≥n de archivo
    if not file_path.exists():
        raise FileNotFoundError(f"Archivo no encontrado: {file_path}")
    
    # 2. Validaci√≥n de formato
    if file_path.suffix.lower() not in ['.csv', '.tsv', '.txt']:
        raise ValueError(f"Formato no soportado: {file_path.suffix}")
    
    # 3. Detecci√≥n de encoding
    for encoding in ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            logger.info(f"Archivo cargado con encoding: {encoding}")
            return df
        except UnicodeDecodeError:
            continue
    
    # 4. Fallback con manejo de errores
    return pd.read_csv(file_path, encoding='utf-8', errors='replace')
```

**Datos de muestra disponibles:**
```python
def create_sample_dataset(self, dataset_type="sales"):
    if dataset_type == "sales":
        # Dataset de ventas con 1000 filas
        # Columnas: Date, Product, Category, Price, Quantity, Customer_ID, Sales_Rep
        
    elif dataset_type == "customers":
        # Dataset de clientes con 500 filas
        # Columnas: Customer_ID, Name, Age, City, Segment, Registration_Date
        
    elif dataset_type == "products":
        # Dataset de productos con 100 filas
        # Columnas: Product_ID, Product_Name, Category, Price, Stock, Supplier
```

#### **StatsProcessor** - Motor de An√°lisis Estad√≠stico

**Capacidades:**
- Estad√≠sticas descriptivas completas
- An√°lisis de correlaciones
- Detecci√≥n de outliers
- An√°lisis de distribuciones
- Comparaciones entre grupos

#### **VisualizationProcessor** - Generaci√≥n de Gr√°ficos

**Tipos de visualizaciones:**
- Histogramas y distribuciones
- Scatter plots y correlaciones
- Series temporales
- Gr√°ficos de barras y comparaciones

### üõ†Ô∏è Herramientas de An√°lisis (DataAnalysisTools)

#### **Quick Profile** - Perfil R√°pido del Dataset
```python
def quick_profile(self, df: pd.DataFrame) -> Dict[str, Any]:
    profile = {
        'basic_info': {
            'rows': len(df),
            'columns': len(df.columns),
            'size_mb': df.memory_usage(deep=True).sum() / 1024**2,
            'columns_list': list(df.columns)
        },
        'data_types': {
            'numeric': len(df.select_dtypes(include=[np.number]).columns),
            'categorical': len(df.select_dtypes(include=['object']).columns),
            'datetime': len(df.select_dtypes(include=['datetime64']).columns)
        },
        'missing_data': {
            'total_missing': df.isnull().sum().sum(),
            'percentage_missing': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        }
    }
    
    # An√°lisis detallado por columna
    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            col_info = {
                'min': df[col].min(),
                'max': df[col].max(), 
                'mean': df[col].mean(),
                'median': df[col].median(),
                'std': df[col].std()
            }
        elif df[col].dtype == 'object':
            col_info = {
                'most_frequent': df[col].mode().iloc[0],
                'unique_values': df[col].nunique(),
                'top_values': df[col].value_counts().head().to_dict()
            }
```

### üîÑ Workflows con LangGraph

El sistema usa **LangGraph** para orquestar el flujo de an√°lisis de manera estructurada:

```python
def _build_graph(self) -> StateGraph:
    workflow = StateGraph(DataAnalysisState)
    
    # Nodos del workflow
    workflow.add_node("load_data", self.load_data_node)
    workflow.add_node("validate_data", self.validate_data_node) 
    workflow.add_node("analyze_data", self.analyze_data_node)
    workflow.add_node("create_visualizations", self.create_visualizations_node)
    workflow.add_node("generate_report", self.generate_report_node)
    
    # Flujo secuencial
    workflow.add_edge(START, "load_data")
    workflow.add_edge("load_data", "validate_data")
    workflow.add_edge("validate_data", "analyze_data") 
    workflow.add_edge("analyze_data", "create_visualizations")
    workflow.add_edge("create_visualizations", "generate_report")
    workflow.add_edge("generate_report", END)
```

**Cada nodo tiene responsabilidades espec√≠ficas:**

1. **load_data_node**: Carga datos desde archivo o crea muestras
2. **validate_data_node**: Verifica calidad y estructura de datos
3. **analyze_data_node**: Ejecuta an√°lisis estad√≠sticos
4. **create_visualizations_node**: Genera gr√°ficos si es necesario
5. **generate_report_node**: Compila resultados en respuesta final

### üéØ Flujo Completo de una Consulta

**Ejemplo paso a paso:**
```
Usuario: "Analiza las ventas por regi√≥n en el archivo ventas_2024.csv"

1. RECEPCI√ìN (PandasAgent.process)
   ‚îú‚îÄ‚îÄ Input: "Analiza las ventas por regi√≥n en el archivo ventas_2024.csv"
   ‚îî‚îÄ‚îÄ Parseado: message="Analiza...", file_path="ventas_2024.csv"

2. CLASIFICACI√ìN (_classify_request)
   ‚îú‚îÄ‚îÄ Detecta palabras: ["analiza", "archivo", "csv"]
   ‚îî‚îÄ‚îÄ Tipo determinado: "analyze_data" 

3. CARGA DE DATOS (_handle_data_loading)
   ‚îú‚îÄ‚îÄ CSVProcessor.load_from_file("ventas_2024.csv")
   ‚îú‚îÄ‚îÄ Detecci√≥n encoding: UTF-8 ‚úì
   ‚îú‚îÄ‚îÄ DataFrame creado: 1000 filas, 7 columnas
   ‚îî‚îÄ‚îÄ DatasetInfo generado con metadatos

4. AN√ÅLISIS (_handle_data_analysis)
   ‚îú‚îÄ‚îÄ StatsProcessor.descriptive_stats()
   ‚îÇ   ‚îú‚îÄ‚îÄ Media de ventas por regi√≥n
   ‚îÇ   ‚îú‚îÄ‚îÄ Totales por regi√≥n  
   ‚îÇ   ‚îî‚îÄ‚îÄ Distribuci√≥n regional
   ‚îú‚îÄ‚îÄ StatsProcessor.correlation_analysis()
   ‚îÇ   ‚îî‚îÄ‚îÄ Correlaciones entre variables
   ‚îî‚îÄ‚îÄ Resultados compilados

5. RESPUESTA FINAL
   ‚îú‚îÄ‚îÄ Formateo de resultados en lenguaje natural
   ‚îú‚îÄ‚îÄ Metadatos para uso posterior
   ‚îî‚îÄ‚îÄ AgentResponse retornado al usuario
```

### üíæ Gesti√≥n de Estado y Memoria

```python
class DataAnalysisState(BaseModel):
    # Entrada del usuario  
    query: str = "Consulta original"
    session_id: str = "ID de sesi√≥n √∫nico"
    
    # Informaci√≥n del dataset
    dataset_info: Dict[str, Any] = "Metadatos del dataset"
    dataframe_id: Optional[str] = "ID del DataFrame cargado"
    
    # Estado del an√°lisis
    analysis_type: Optional[AnalysisType] = "Tipo de an√°lisis detectado"
    analysis_result: str = "Resultado del an√°lisis"
    
    # Contexto conversacional
    conversation_history: List[str] = "Historial de consultas"
    
    # Resultado final
    final_response: str = "Respuesta formateada para el usuario"
    
    # Objetos internos (no serializables)
    dataframe: Optional[pd.DataFrame] = "DataFrame actual en memoria"
    pandas_agent: Optional[Any] = "Agente pandas activo"
```

### üîß Extensibilidad del Sistema

El sistema est√° dise√±ado para ser f√°cilmente extensible:

**Para agregar nuevos tipos de datos:**
```python
# 1. Crear nuevo procesador
class JSONProcessor:
    def load_from_json_db(self, dataset_name):
        # L√≥gica de carga desde JSON
        
# 2. Registrar en PandasAgent
def __init__(self):
    self.json_processor = JSONProcessor()
    
# 3. Modificar clasificaci√≥n de requests
def _classify_request(self, message):
    if "dataset" in message.lower():
        return "load_json_dataset"
```

**Para agregar nuevos tipos de an√°lisis:**
```python
# 1. Crear m√©todo en StatsProcessor
def advanced_ml_analysis(self, df):
    # An√°lisis de machine learning
    
# 2. Agregar a _handle_data_analysis
ml_analysis = await asyncio.to_thread(
    self.stats_processor.advanced_ml_analysis,
    self.current_dataframe
)
analyses['machine_learning'] = ml_analysis
```

## Limitaciones

- Requiere API key de OpenAI
- Limitado a an√°lisis que pandas puede realizar
- No genera visualizaciones autom√°ticamente
- Memoria de sesi√≥n no persiste entre reinicios
- Actualmente solo soporta archivos CSV/TSV
- No incluye an√°lisis de machine learning avanzado
- Visualizaciones requieren configuraci√≥n adicional

## Extensiones Futuras

- [ ] Soporte para bases de datos SQL
- [ ] Generaci√≥n autom√°tica de gr√°ficos
- [ ] Persistencia de memoria en disco
- [ ] Integraci√≥n con Jupyter Notebooks
- [ ] Exportaci√≥n de resultados a PDF/Excel
- [ ] An√°lisis de texto y NLP
- [ ] Machine Learning b√°sico

## Troubleshooting Detallado

### üîß Problemas Comunes y Soluciones

#### **1. Errores de Configuraci√≥n**

**Error: OPENAI_API_KEY no encontrada**
```bash
# S√≠ntoma
Error: "OpenAI API key not found"

# Diagn√≥stico
1. Verificar archivo .env existe en directorio ra√≠z
2. Confirmar formato correcto en .env:
   OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx
3. Verificar que no hay espacios extra alrededor del =
4. Confirmar que el archivo .env no tiene extensi√≥n .txt oculta

# Soluci√≥n
1. Crear/editar archivo .env en la ra√≠z del proyecto
2. Agregar la l√≠nea: OPENAI_API_KEY=tu_api_key_real
3. Reiniciar la aplicaci√≥n
4. Verificar carga: print(os.getenv('OPENAI_API_KEY'))
```

**Error: Import pandas could not be resolved**
```bash
# S√≠ntoma
ImportError: No module named 'pandas'

# Diagn√≥stico
1. Verificar entorno virtual activado
2. Confirmar instalaci√≥n de dependencias

# Soluci√≥n
pip install -r requirements.txt
# O espec√≠ficamente:
pip install pandas numpy langchain langchain-experimental langgraph
```

#### **2. Errores de Carga de Datos**

**Error: Archivo CSV no encontrado**
```python
# S√≠ntoma
FileNotFoundError: Archivo no encontrado: datos.csv

# Diagn√≥stico
print(os.getcwd())  # Verificar directorio actual
print(os.path.exists("datos.csv"))  # Verificar existencia

# Soluciones
1. Usar ruta absoluta:
   "/Users/usuario/Documents/datos.csv"
2. Verificar ubicaci√≥n relativa al script
3. Usar Path objects:
   from pathlib import Path
   file_path = Path("data") / "datos.csv"
```

**Error: Encoding de caracteres**
```python
# S√≠ntoma
UnicodeDecodeError: 'utf-8' codec can't decode byte

# El sistema maneja esto autom√°ticamente, pero si persiste:
# Soluci√≥n manual
df = pd.read_csv("archivo.csv", encoding='latin1')
# O
df = pd.read_csv("archivo.csv", encoding='cp1252')
```

**Error: Formato de datos inv√°lido**
```python
# S√≠ntoma
pandas.errors.EmptyDataError: No columns to parse from file

# Diagn√≥stico
1. Verificar que el archivo no est√© vac√≠o
2. Confirmar que tiene headers
3. Verificar delimitadores correctos

# Soluci√≥n
# Vista previa del archivo
with open("datos.csv", 'r') as f:
    print(f.read(500))  # Primeros 500 caracteres
```

#### **3. Errores de An√°lisis**

**Error: "Primero debes cargar datos"**
```python
# S√≠ntoma
"Primero debes cargar datos antes de analizarlos."

# Causa
El agent no tiene un DataFrame cargado en current_dataframe

# Soluci√≥n
1. Cargar datos expl√≠citamente:
   await agent.respond("Carga datos de muestra", session)
2. Verificar carga exitosa:
   if agent.current_dataframe is not None:
       print(f"Datos cargados: {agent.current_dataframe.shape}")
```

**Error: Columna no encontrada**
```python
# S√≠ntoma
KeyError: 'columna_inexistente'

# Diagn√≥stico
print(agent.current_dataframe.columns.tolist())

# Soluci√≥n
1. Verificar nombres exactos de columnas
2. Usar consultas gen√©ricas:
   "¬øQu√© columnas tiene el dataset?"
3. Revisar case-sensitivity
```

#### **4. Errores de Memoria y Performance**

**Error: Memoria insuficiente**
```python
# S√≠ntoma
MemoryError: Unable to allocate array

# Diagn√≥stico
print(f"Tama√±o del dataset: {agent.current_dataframe.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# Soluciones
1. Procesar en chunks:
   chunk_size = 10000
   for chunk in pd.read_csv("archivo_grande.csv", chunksize=chunk_size):
       # Procesar chunk
       
2. Optimizar tipos de datos:
   df['columna_int'] = df['columna_int'].astype('int32')  # En lugar de int64
   df['columna_cat'] = df['columna_cat'].astype('category')
   
3. Usar sample para pruebas:
   df_sample = df.sample(n=1000)  # Solo 1000 filas para prueba
```

**Error: Timeouts en an√°lisis**
```python
# S√≠ntoma
asyncio.TimeoutError: Analysis took too long

# Soluciones
1. Usar datasets m√°s peque√±os para pruebas
2. Limitar an√°lisis a columnas espec√≠ficas:
   await agent.respond("Analiza solo las columnas precio y cantidad", session)
3. Dividir an√°lisis complejos en pasos:
   await agent.respond("Dame estad√≠sticas b√°sicas", session)
   await agent.respond("Ahora analiza correlaciones", session)
```

### üõ°Ô∏è Mejores Pr√°cticas

#### **1. Preparaci√≥n de Datos**

**Estructura de archivos CSV recomendada:**
```csv
# ‚úÖ Buena estructura
fecha,producto,ventas,region
2024-01-01,Laptop,1200.50,Norte
2024-01-02,Mouse,25.90,Sur

# ‚ùå Evitar
# Fecha,Producto con espacios,Ventas ($),Regi√≥n/Zona
# 01/01/2024,"Laptop Gaming ""Pro""","$1,200.50",Norte/Centro
```

**Convenciones de nombres:**
```python
# ‚úÖ Recomendado
columnas = [
    'fecha',           # Sin espacios
    'producto_id',     # Separadores con underscore
    'precio_unitario', # Descriptivo y claro
    'cantidad',        # Sin abreviaciones confusas
    'descuento_pct'    # Unidades claras
]

# ‚ùå Evitar
columnas = [
    'Fecha de Venta',  # Espacios
    'Prod ID',         # Espacios y abreviaci√≥n
    'Precio ($)',      # Caracteres especiales
    'Cant.',           # Abreviaci√≥n ambigua
    'Desc%'            # S√≠mbolo especial
]
```

**Validaci√≥n previa:**
```python
# Validar antes de an√°lisis principal
def validate_csv_before_analysis(file_path):
    # 1. Verificar estructura b√°sica
    preview = pd.read_csv(file_path, nrows=5)
    print(f"Columnas: {preview.columns.tolist()}")
    print(f"Tipos: {preview.dtypes.to_dict()}")
    
    # 2. Verificar datos faltantes
    missing_pct = (preview.isnull().sum() / len(preview)) * 100
    print(f"Datos faltantes: {missing_pct.to_dict()}")
    
    # 3. Verificar duplicados en preview
    duplicates = preview.duplicated().sum()
    print(f"Duplicados en muestra: {duplicates}")
    
    return True if duplicates == 0 and missing_pct.max() < 50 else False
```

#### **2. Optimizaci√≥n de Consultas**

**Consultas efectivas:**
```python
# ‚úÖ Espec√≠ficas y claras
"¬øCu√°l es el precio promedio de laptops en la regi√≥n Norte?"
"Compara las ventas de enero vs febrero 2024"
"¬øQu√© vendedor tiene mayor volumen de ventas?"

# ‚ùå Ambiguas o muy generales
"Analiza todo"
"¬øQu√© pasa con los datos?"
"Hazme un an√°lisis completo"
```

**Uso de contexto conversacional:**
```python
# ‚úÖ Aprovechar memoria de sesi√≥n
session = "analisis_ventas"
await agent.respond("¬øCu√°l es el producto m√°s vendido?", session)
await agent.respond("¬øEn qu√© regi√≥n se vende m√°s ese producto?", session)  # "ese producto" se entiende
await agent.respond("¬øCu√°l es la tendencia de ventas para esa combinaci√≥n?", session)  # Contexto acumulado

# ‚ùå Repetir informaci√≥n
await agent.respond("¬øCu√°l es el producto m√°s vendido?", session)
await agent.respond("¬øEn qu√© regi√≥n se vende m√°s el producto m√°s vendido?", session)  # Redundante
```

#### **3. Gesti√≥n de Sesiones**

**Organizaci√≥n por prop√≥sito:**
```python
# ‚úÖ Sesiones espec√≠ficas
sessions = {
    "analisis_ventas_q1": "An√°lisis del primer trimestre",
    "comparativa_productos": "Comparaci√≥n entre productos",
    "tendencias_anuales": "An√°lisis de tendencias a√±o completo",
    "segmentacion_clientes": "An√°lisis de segmentos de clientes"
}

# ‚ùå Sesiones mezcladas
session = "analisis_general"  # Todo mezclado, pierde contexto
```

**Limpieza de memoria:**
```python
# Limpiar sesiones cuando no se necesiten
agent.clear_session_memory("analisis_temporal_old")

# Ver sesiones activas
active_sessions = agent.get_available_sessions()
print(f"Sesiones activas: {active_sessions}")
```

#### **4. Monitoreo y Debugging**

**Logging personalizado:**
```python
import logging

# Configurar logging para debugging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Ver logs del agente
logger = logging.getLogger('ai_agents.agents.data_analysis')
logger.setLevel(logging.DEBUG)
```

**Verificaci√≥n de estado:**
```python
# Funci√≥n de diagn√≥stico personalizada
def debug_agent_state(agent):
    print("=== ESTADO DEL AGENTE ===")
    print(f"DataFrame cargado: {agent.current_dataframe is not None}")
    if agent.current_dataframe is not None:
        print(f"Forma del dataset: {agent.current_dataframe.shape}")
        print(f"Columnas: {agent.current_dataframe.columns.tolist()}")
        print(f"Memoria usada: {agent.current_dataframe.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    print(f"Dataset info: {agent.current_dataset_info is not None}")
    print(f"√öltimo an√°lisis: {agent.last_analysis_result is not None}")
    
    sessions = agent.get_available_sessions()
    print(f"Sesiones activas: {len(sessions)}")
    for session in sessions:
        history = agent.get_session_history(session)
        print(f"  - {session}: {len(history)} mensajes")

# Uso
debug_agent_state(agent)
```

### üìã Checklist de Verificaci√≥n

#### **Antes de usar el agente:**
- [ ] ‚úÖ Variable OPENAI_API_KEY configurada
- [ ] ‚úÖ Dependencias instaladas (`pip install -r requirements.txt`)
- [ ] ‚úÖ Archivo de datos preparado y validado
- [ ] ‚úÖ Nombres de columnas sin espacios ni caracteres especiales
- [ ] ‚úÖ Datos en formato UTF-8 o encoding conocido

#### **Durante el an√°lisis:**
- [ ] ‚úÖ Sesiones organizadas por prop√≥sito
- [ ] ‚úÖ Consultas espec√≠ficas y claras
- [ ] ‚úÖ Aprovechamiento de contexto conversacional
- [ ] ‚úÖ Verificaci√≥n de resultados intermedios
- [ ] ‚úÖ Monitoreo de memoria y performance

#### **Despu√©s del an√°lisis:**
- [ ] ‚úÖ Resultados validados manualmente (sample)
- [ ] ‚úÖ Sesiones innecesarias limpiadas
- [ ] ‚úÖ Insights documentados para uso futuro
- [ ] ‚úÖ Datos sensibles removidos de memoria

### üÜò Soporte y Recursos Adicionales

**Logs de sistema:**
```bash
# Ubicaci√≥n de logs
data/logs/YYYY-MM-DD/
‚îú‚îÄ‚îÄ conversation_logs/
‚îÇ   ‚îú‚îÄ‚îÄ session_id_TIMESTAMP.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ system_logs/
    ‚îú‚îÄ‚îÄ agent_errors.log
    ‚îî‚îÄ‚îÄ performance.log
```

**Comandos de diagn√≥stico:**
```python
# Test de conectividad con OpenAI
import openai
try:
    client = openai.OpenAI()
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "test"}],
        max_tokens=5
    )
    print("‚úÖ Conexi√≥n OpenAI OK")
except Exception as e:
    print(f"‚ùå Error OpenAI: {e}")

# Test de pandas
try:
    import pandas as pd
    df = pd.DataFrame({'test': [1, 2, 3]})
    print("‚úÖ Pandas OK")
except Exception as e:
    print(f"‚ùå Error Pandas: {e}")
```
